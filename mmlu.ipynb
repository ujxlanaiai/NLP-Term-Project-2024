{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage.embeddings import UpstageEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPSTAGE_API_KEY = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions and neccesary templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split the prompt into the question and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_options_answer(prompt):\n",
    "    \"\"\"\n",
    "    Function that dynamically extracts questions, options, and answers\n",
    "    \"\"\"\n",
    "    options_start = re.search(r\"\\([A-Z]\\)\\s\", prompt)  # Find the location of the first option\n",
    "    \n",
    "    question = prompt[:options_start.start()].strip() if options_start else None\n",
    "    \n",
    "    options_match = re.findall(r\"\\([A-Z]\\)\\s.*?(?=\\n|$)\", prompt, re.DOTALL)\n",
    "    options = options_match if options_match else []\n",
    "    \n",
    "    answer_match = re.search(r\"\\[ANSWER\\]:\\s*\\((.)\\)\", prompt)\n",
    "    correct_answer = answer_match.group(1).strip() if answer_match else None\n",
    "    \n",
    "    return question, options, correct_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLM to generate category of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "llm_category = ChatUpstage(api_key = UPSTAGE_API_KEY)\n",
    "category_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are given a question and its answer options. \n",
    "    Your task is to identify the category to which the question belongs to, out of the following categories:\n",
    "    - Law  \n",
    "    - Psychology  \n",
    "    - Business  \n",
    "    - Philosophy  \n",
    "    - History  \n",
    "    \n",
    "    Instructions:\n",
    "    1. Read the question and options carefully.\n",
    "    2. Identify the category that best fits the question.\n",
    "    3. Return only the category name (Law/Psycology/Business/Philosophy/History).\n",
    "    ---\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Options: \n",
    "    {options}\n",
    "    \"\"\"\n",
    ")\n",
    "category_chain = category_prompt_template | llm_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate questions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Upstage embedding model\n",
    "embedding_model = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-embedding-1-large-query\"  # Specify the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_embeddings(question, embedding_model):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a question and its words.\n",
    "    \"\"\"\n",
    "    question_embedding = embedding_model.embed_documents([question])[0]\n",
    "\n",
    "    words = question.split()\n",
    "    word_embeddings = embedding_model.embed_documents(words)\n",
    "\n",
    "    return question_embedding, words, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_keywords(question_embedding, words, word_embeddings, top_n=3):\n",
    "    \"\"\"\n",
    "    Extract top N keywords from the question based on cosine similarity with the question embedding.\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity([question_embedding], word_embeddings)[0]\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    top_keywords = [words[i] for i in top_indices]\n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to search keywords in wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent=\"MyApp/1.0 (threwja@gamil.com)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(keyword):\n",
    "    \"\"\"\n",
    "    Search Wikipedia for a given keyword and return the first paragraph of the summary.\n",
    "    Args:\n",
    "        keyword (str): The keyword to search on Wikipedia.\n",
    "    Returns:\n",
    "        str: The first paragraph of the Wikipedia page summary or an error message.\n",
    "    \"\"\"\n",
    "    page = wiki_wiki.page(keyword)\n",
    "    if page.exists():\n",
    "        # Return the first paragraph of the summary\n",
    "        return page.summary.split('\\n')[0]\n",
    "    else:\n",
    "        return f\"No information found for {keyword}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate context from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_from_wikipedia(keywords):\n",
    "    \"\"\"\n",
    "    Generate a context by fetching information from Wikipedia for the given keywords.\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for keyword in keywords:\n",
    "        summary = search_wikipedia(keyword)\n",
    "        context.append(f\"Keyword: {keyword}\\n{summary}\")\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_from_wikipedia(keywords):\n",
    "    \"\"\"\n",
    "    Fetch Wikipedia context for the given keywords.\n",
    "    If a keyword has no result, try its lemmatized form.\n",
    "    Args:\n",
    "        keywords (list): List of keywords to search on Wikipedia.\n",
    "    Returns:\n",
    "        str: Context string containing summaries for the keywords.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Lemmatize the keyword\n",
    "        lemmatized_keyword = lemmatizer.lemmatize(keyword)\n",
    "\n",
    "        # Search for the lemmatized keyword first\n",
    "        result = search_wikipedia(lemmatized_keyword)\n",
    "        \n",
    "        # If no information is found, fall back to the original keyword\n",
    "        if \"No information found\" in result:\n",
    "            result = search_wikipedia(keyword)\n",
    "\n",
    "        # Add the result to the context\n",
    "        context += f\"Keyword: {keyword}\\n{result}\\n\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_llm = ChatUpstage(api_key = UPSTAGE_API_KEY)\n",
    "mmlu_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a highly knowledgeable expert in {category}, renowned for precision in decision-making and logical reasoning.\n",
    "    Your task is to analyze the following multiple-choice question and select the single best option based on your expertise.\n",
    "\n",
    "    ### Instructions:\n",
    "    1. Carefully read and understand the provided question and options.\n",
    "    2. Use logical reasoning and domain-specific knowledge to identify the most accurate answer.\n",
    "    3. Provide your answer in the following format:\n",
    "       [ANSWER]: (option letter)\n",
    "\n",
    "    ### Guidelines for Your Response:\n",
    "    - Ensure your response is concise and strictly follows the requested format.\n",
    "    - If the question is ambiguous or incomplete, explain why before providing an answer.\n",
    "    - Only choose one option that best fits the context of the question.\n",
    "\n",
    "    ### Format for Your Response:\n",
    "    1. **Keywords Identified**: List the key concepts or keywords from the question.\n",
    "    2. **Reasoning**: Briefly explain how the keywords help in selecting the best answer.\n",
    "    3. **Final Answer**: Provide the single best option in the following format: [ANSWER]: (option letter)\n",
    "\n",
    "    ---\n",
    "    **Question:** \n",
    "    {question}\n",
    "\n",
    "    **Options:** \n",
    "    {options}\n",
    "\n",
    "    ---\n",
    "    \"\"\"\n",
    ")\n",
    "mmlu_chain = mmlu_prompt_template | mmlu_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate category of the question\n",
    "def generate_category_for_question(question, options):\n",
    "    category_response = category_chain.invoke({\"question\": question, \"options\": options}).content\n",
    "    return category_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompts the model for a response 5 times and chooses the most common returned response. \n",
    "\n",
    "- If there is no response returned, it will reprompt the model for another 5 responses, until the model returns a response.\n",
    "\n",
    "- If maximum number of retries is exceeded, then use logical reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns most common answer if consistency is above threshold\n",
    "def get_consistent_answer(responses, consistency_threshold=0.8):\n",
    "    \n",
    "    answer_counts = {}\n",
    "\n",
    "    # Count the number of times each answer appears in the list of responses\n",
    "    for response in responses:\n",
    "        match = re.search(r\"\\[ANSWER\\]:\\s*\\(([A-Z])\\)\", response)\n",
    "        if match:\n",
    "            answer = match.group(1)\n",
    "            answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "    \n",
    "    # Calculate the total number of responses and the most common answer\n",
    "    total_responses = len(responses)\n",
    "    most_common_answer = max(answer_counts, key=answer_counts.get, default=None)\n",
    "    most_common_count = answer_counts.get(most_common_answer, 0)\n",
    "\n",
    "    # Check if the most common answer is consistent enough (above the threshold)\n",
    "    if most_common_count / total_responses >= consistency_threshold:\n",
    "        return f\"[ANSWER]: ({most_common_answer})\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(context, question, options, category, max_retries=3):\n",
    "    consistent_answer = None\n",
    "    retries = 0\n",
    "\n",
    "    while consistent_answer is None and retries < max_retries:\n",
    "        individual_responses = []\n",
    "\n",
    "        # Generate 5 independent responses\n",
    "        for _ in range(5):  \n",
    "            response = mmlu_chain.invoke({\"context\": context, \"question\": question, \"options\": options, \"category\": category, \"temperature\": 0}).content\n",
    "            individual_responses.append(response)\n",
    "\n",
    "        # Get the most consistent answer\n",
    "        consistent_answer = get_consistent_answer(individual_responses)\n",
    "\n",
    "        # Printing the generated responses and the consistent answer\n",
    "        print(f\"Generated Responses: {individual_responses}\")\n",
    "        print(f\"Consistent Answer: {consistent_answer}\")\n",
    "        \n",
    "        retries += 1\n",
    "\n",
    "    if consistent_answer is None:\n",
    "        print(\"No consistent answer found. Using the most recent logical response.\")\n",
    "        logical_response = individual_responses[-1]\n",
    "        match = re.search(r\"\\[ANSWER\\]:\\s*\\(([A-Z])\\)\", logical_response)\n",
    "        if match:\n",
    "            consistent_answer = f\"[ANSWER]: ({match.group(1)})\"\n",
    "        else:\n",
    "            print(\"No valid answer found in the logical reasoning response.\")\n",
    "            consistent_answer = None\n",
    "\n",
    "    return consistent_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mmlu(sampled_prompts):\n",
    "    responses = []\n",
    "\n",
    "    for prompt in sampled_prompts:\n",
    "        # Split the prompt into question and options\n",
    "        question, options_list, _ = extract_question_options_answer(prompt)\n",
    "        options = \"\\n\".join(options_list)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Options:\", options)\n",
    "\n",
    "        # Generate the category for the question\n",
    "        category = generate_category_for_question(question, options)\n",
    "        print(\"Category:\", category)\n",
    "\n",
    "        # Step 1: Generate question embedding and word embeddings\n",
    "        question_embedding, words, word_embeddings = generate_question_embeddings(\n",
    "            question, embedding_model\n",
    "        )\n",
    "\n",
    "        # Step 2: Extract top 3 keywords\n",
    "        top_keywords = extract_top_keywords(question_embedding, words, word_embeddings)\n",
    "        print(\"Top Keywords:\", top_keywords)\n",
    "\n",
    "        # Step 3: Fetch Wikipedia context for the keywords\n",
    "        wikipedia_context = generate_context_from_wikipedia(top_keywords)\n",
    "        print(\"Wikipedia Context:\\n\", wikipedia_context)\n",
    "\n",
    "        # Step 4: Use context in the LLM call\n",
    "        context = f\"### Context:\\n{wikipedia_context}\"\n",
    "        print(\"Final Context:\\n\", context)\n",
    "\n",
    "        # Step 5: Call get_answer with separate context\n",
    "        consistent_answer = get_answer(context, question, options, category)\n",
    "\n",
    "        print(\"Response:\", consistent_answer)\n",
    "        print(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "        responses.append(consistent_answer)\n",
    "\n",
    "    return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
